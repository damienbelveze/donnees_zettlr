---
title: ré-entraînement à partir du feedback humain
subtitle: 
id: 20241113_ré-entraînement
author: Damien Belvèze
date: 2024-11-13
link_citations: true
bibliography: biblio/Obsidian.bib
biblio_style: csl\ieee.csl
aliases:
  - Reinforcement Learning From Human Feedback
  - apprentissage par renforcement avec retour humain
tags:
  - IA
---
permet d'aligner l'entraînement automatique des intelligences artificielles avec les attentes humaines. 

Le réentraînement a lieu avant la mise sur le marché et se poursuit après. Les LLM mis sur le marché ne semblent pas selon Olivier Ertzscheid avoir beaucoup progressé depuis novembre 2022 (lancement de ChatGPT) en terme de ré-entraînement pre-release : 

> **De la même manière le lancement de Lucie** eut donc à faire face à ses oeufs non pas de mouton mais de vache, et à son épisode d’apologie des crimes nazis (et/ou de négationnisme). Plus quelques autres naufrages mathématiques (la racine carrée d’une chèvre ou bien encore des opérations de calcul de factorisation niveau 6ème) que là encore la totalité des autres IA conversationnelles lancées rencontrèrent à leurs débuts. La naufrage de Lucie n’est donc pas tant un naufrage technique (ces bugs seront bien sûr corrigés) qu’un naufrage marketing eu égard au fait que précisément aujourd’hui et déjà depuis un an, tout lancement d’une IA conversationnelle s’accompagne de la correction de ces bugs « élémentaires », en tout cas dans la version grand public.

[[@ertzscheidLucieDansChoux2025]]


$\newline$
# bibliographie
$\newline$






