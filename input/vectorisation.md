---
title: vectorisation
subtitle:
id: 20250305_vectorisation
author: Damien Belvèze
date: 2025-03-05
link_citations: true
bibliography: biblio/Obsidian.bib
biblio_style: csl\ieee.csl
aliases:
  - embeddings
tags:
  - IA
---
La vectorisation de mots (word embeddings) consiste à donner une représentation numérique d'un mot ou d'un token et de leurs relations (voir [[grands modèles de langage|LLM]])

>In many [[NLP]] tasks, [[tokens]] are converted into numerical vectors using techniques like Bag of Words (BoW), TF-IDF (Term Frequency-Inverse Document Frequency), or **word embeddings** (like Word2Vec, GloVe). This process turns text data into numbers that machine learning models can understand and work with. (source : https://medium.com/the-research-nest/explained-tokens-and-embeddings-in-llms-69a16ba5db33)




$\newline$
# bibliographie
$\newline$






