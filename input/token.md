---
title: token
subtitle: 
id: 20231215_token
author: Damien Belvèze
date: 2023-12-15
link_citations: true
bibliography: biblio/Obsidian.bib
biblio_style: csl\ieee.csl
aliases: 
tags:
  - IA
---
ne pas confondre avec [[tokénisme]] en socio qui consiste à présenter un cas isolé et exceptionnel comme cas général pour maquer un privilège

Dans un [[grands modèles de langage|large language model]], la [[tokenisation]] consiste à réduire l'input et l'output en petites unités sémantique ; 
Par exemple, hello, world contient habituellement 3 tokens

"Hello"+","+"world"

Pour savoir combien de tokens comporte un texte, utiliser le service tokenizer d'OpenAI : https://platform.openai.com/tokenizer

un token traduit à peu près 3/4 d'un mot. Pour une langue naturelle, on doit compter sur à peu près 32000 tokens

livre comporte deux tokens, un pour désigner l'objet, l'autre pour désigner le singulier





$\newline$
# bibliographie
$\newline$






