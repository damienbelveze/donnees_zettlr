---
title: La méthode CRAAP et les approches de type checklist dans l'évaluation de l'information
subtitle: 
id: 20240110_méthode CRAAP
author: Damien Belvèze
date: 2024-01-10
link_citations: true
bibliography: biblio/Obsidian.bib
biblio_style: csl\ieee.csl
aliases: 
tags:
---


- Currency (l'information est-elle à jour)
- Revelance (l'information est-elle pertinente par rapport à mon besoin)
- Authority (l'information provient-elle d'une personne qui a une [[expertise]] sur le sujet)
- Accuracy (l'information est-elle exacte)
- Purpose (dans quel but l'information est-elle publiée, diffusée)

Cette méthode a été conçue à l'avènement du web pour aider les étudiants à évaluer l'information accessible depuis leur navigateur. Il existe d'autres méthodes de ce type, notamment la [[méthode ESCAPE]]


La méthode CRAAP a montré certaines limites qui sont détaillées dans les articles suivants résumés par Lane Wilkinson dans un post intitulé [LOEX 2017: Teaching Popular Source Evaluation in an Era of Fake News, [[Post-vérité|post-truth]], and Confirmation Bias](https://senseandreference.wordpress.com/2017/06/02/loex2017/). Lane Wilkinson s'inspire des travaux de Mike Caufield sur le sujet :

>However, the CRAAP checklist has faced substantial criticism for being insufficient and inadequate in addressing the complexities of the present-day information landscape ([Caulfield, 2016](https://www-sciencedirect-com.passerelle.univ-rennes1.fr/science/article/pii/S0099133323001106?dgcid=rss_sd_all#bb0055); [Crum, 2017](https://www-sciencedirect-com.passerelle.univ-rennes1.fr/science/article/pii/S0099133323001106?dgcid=rss_sd_all#bb7020)) This pertains to the fact that the CRAAP test was initially developed rooted in an era during the shift from Web 1.0 to [Web 2.0](https://www-sciencedirect-com.passerelle.univ-rennes1.fr/topics/social-sciences/web-2-0 "Learn more about Web 2.0 from ScienceDirect's AI-generated Topic Pages"); during this period the world underwent a significant transformation in the digital landscape, most online content was primarily intended for consumption rather than active interaction, modification, sharing, and when online information is treated as equivalent to printed texts and emphasizing careful reading for evaluation ([Valenza, 2020](https://www-sciencedirect-com.passerelle.univ-rennes1.fr/science/article/pii/S0099133323001106?dgcid=rss_sd_all#bb0380)). [Bull (2021)](https://www-sciencedirect-com.passerelle.univ-rennes1.fr/science/article/pii/S0099133323001106?dgcid=rss_sd_all#bb0050) point out that although the CRAAP test is designed to lessen cognitive overload, it might have the opposite effect, exacerbating cognitive burden. Consequently, students may end up making sub-optimal decisions concerning the credibility of sources, especially in intricate and interconnected online environments. [Seeber (2017)](https://www-sciencedirect-com.passerelle.univ-rennes1.fr/science/article/pii/S0099133323001106?dgcid=rss_sd_all#bb7045) further added that the CRAAP test approach relies on simplistic and binary criteria for evaluating sources. Instead of encouraging deep critical thinking and nuanced analysis, it tends to categorize sources into rigid and dichotomous classifications. The CRAAP test may not fully capture the complexities of information evaluation in real-world scenarios where truth, credibility, and reliability are often multifaceted and require more comprehensive assessment. It's approaches are insufficient when confronted with the vast expanse of the web, teeming with hoaxes, deep fakes, cloaked sites, source hackers, and scammers. The complexity and diversity of online content render such guidelines ineffective in addressing the challenges posed by the ever-evolving landscape of misinformation and deceptive sources ([Wineburg et al., 2020](https://www-sciencedirect-com.passerelle.univ-rennes1.fr/science/article/pii/S0099133323001106?dgcid=rss_sd_all#bb0400)).

(source : [[@chyneEvaluationSocialMedia2023]] )

**Currency** : une certaine connaissance du sujet est souvent nécessaire pour savoir si une information est assez récente ou pas. Dans certains domaines, hier est déjà d'un autre âge, tandis qu'en philosophie, Epicure est toujours notre contemporain.

**Relevance**: ce n'est pas un critère de jugement sur la fiabilité de l'information. Mais le lecteur doit régulièrement arbitrer entre pertinence et fiabilité. La pertinence d'une information n'est pas uniquement d'ordre sémantique [[topicalité]]. Par ailleurs, si on rationalise, ne risque t-on pas de trouver plus pertinentes des informations qui vont dans notre sens que dans des informations qui vont à rebours de nos opinions ? 

**authority**([[autorité]]): possible méprise due au [[biais d'autorité]]

**Accuracy**: Comment évaluer les écrits de quelqu'un qui soutient qu'il est expert quand on ne l'est pas soi-même ? (=[[paradoxe de l'expertise]]) voir la note consacrée à l'[[expertise]]

**Purpose**: (à compléter, l'explication de Wilkinson n'apparaît pas très convaincante)

1. L'un des problèmes posés par les [[checklists]] (listes de critères) comme CRAAP ou ESCAPE, c'est qu'elles incitent l'internaute à rester sur la page alors qu'on glane plus d'information souvent en quittant la page pour faire une [[lecture latérale]] (*lateral reading*). Concrètement, selon ces auteurs, les spécialistes de l'évaluation de l'information n'utilisent pas de checklist pour évaluer des sites.
2. Une autre critique vis à vis du test CRAAP est qu'il est centré sur l'imprimé. Dans le monde de l'information imprimée, le fait de circuler autour d'une référence pour vérifier sa plausibilité prend beaucoup de temps. Pour le web, c'est instantané. Le fait de croiser l'info par d'autres infos publiées antérieurement est très rapide
3. Les étudiants trop confiants dans la méthode CRAAP ne pensent pas à mobiliser leur propre savoir pour apprécier si l'information contenue dans le texte est vraisemblable ou pas, comme dans le cas de [l'arbre-pieuvre](https://zapatopi.net/treeoctopus/, un hoax lancé à Singapour afin de mesurer la perméabilité des étudiants aux fausses informations. 

>    In his blog post, Caulfield cited another writer’s illustration of how a [hoax website](https://zapatopi.net/treeoctopus/) about an endangered “tree octopus” would pass a conventional web-literacy checklist. One thing those checklists miss, he argues, is the need for students to apply existing knowledge. The big clue that the website is a hoax isn’t anything about the site per se, he wrote, but “the improbability of a cephalopod making the leap to being an amphibious creature without significant evolutionary changes.” Octopuses, in other words, don’t climb trees.

(source: [[@supianoStudentsFallMisinformation2019]] 


4. Un autre problème a été manifesté par une étude de Marc Meola en 2004. La thèse de Meola est qu'on ne peut réduire l'esprit critique à l'application d'un modèle constitué de critères à vérifier. Cette façon de procéder est trop [[algorithme|algorithmique]] et mécanique pour permettre de faire face aux enjeux liés à l'évaluation de l'information : 

> the cheklist model  \[...] can serve to promote a mechanical and algorithmmic way of evaluation that is at odds with the higher-level judgement and intuition that we presumably seek to cultivate as part of critical thinking

(source: [[@meolaChuckingChecklistContextual2004]])

L'auteur promeut au contraire une approche contextuelle basée sur la comparaison de plusieurs sources (différents sites web ou un site web et un ouvrage), ou la corroboration d'une source par d'autres sources. Il précise que si la méthode de la checklist est promue par les bibliothécaires qui s'intéressent à des documents précis, elle n'est pas appropriée au web où l'information est disséminée de manière incontrôlée entre une grande quantité de sites. La méthode contextuelle ne recueille pas au départ l'assentiment de cette profession pour cette raison qu'elle prend appui sur bien d'autres documents que ceux dont le bibliothécaire se voit le gardien ou l'amabassadeur : 

> althought corroboration is used outside librarianship, librarians generally do not present it as a website evaluation strategy. 

La [[Méthode SIFT]] en élargissant l'investigation à l'ensemble du web est censée corriger les méthodes mentionnées plus haut qui se concentrent trop sur le document lui-même.

5. S'inspirant de la pédagogie de [[Paulo Freire]], Melissa Chomintra considère l'usage de la *checklist* comme relevant d'une pédagogie oppressive (dans sa variante du *banking model*) dans la mesure où l'étudiant ne détermine pas lui-même les critères de cette checklist à l'occasion de la résolution d'un problème : 

> The banking model is a pedagogical practice that sees students as passive recipients of knowledge ; vaults in which knowledge is deposited by teachers and ignores education and knowledge as a process of inquiry

(Freire cité par Chomintra [[@chomintraAssessingUseCritical2023]])

6. Le test CRAAP ou n'importe quel autre check-list fait l'impasse sur les représentations à l'oeuvre dans l'information à évaluer. 

>It requires further investigation to explore why librarians don't talk about representation (i.e., identify whose voices are represented/ omitted/ marginalized)

(source: [[@kozlowska-barriosMediaInformationLiteracy2023]])


En dépit de ces limites, un sondage dont les résultats ont été publiés en mai 2023 montre que les bibliothécaires qui interviennent dans des sessions de "media literacy" utilisent dans 67% des cas un modèle de check-list [[@kozlowska-barriosMediaInformationLiteracy2023]].

Méthode CRAAP à Rennes 2:
![[methode_CRAAP_infographie.pdf]]








